{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">üñºÔ∏è Img2Prompt üìÑ -  BLIP + CLIP | CLIP Interrogator</center>\n<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Stable Diffusion - Image to Prompts</center></p>\n\n***","metadata":{}},{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">(‡≤†‡≤ø‚Å†_‚Å†‡≤†) Overview</center>\n\n<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ This notebook provides guidance on submitting a sample entry for the Stable Diffusion - Image to Prompts contest.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ In order to properly generate embeddings of your predicted prompts, it is crucial that you include the <a href=\"https://www.kaggle.com/datasets/inversion/sentence-transformers-222\"><strong>sentence-transformers-2.2.2</strong></a> dataset.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ In this example, the <a href=\"https://github.com/pharmapsychotic/clip-interrogator\"><strong>CLIP interrogator tool</strong></a> is used to extract text.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\"> üî¥ The new stable diffusion model (Stable Diffusion 2.0-v) at 768x768 resolution has the same number of parameters in the U-Net as 1.5, but uses <a href=\"https://github.com/Stability-AI/stablediffusion#news\"><strong>OpenCLIP-ViT/H</strong></a> as the text encoder and is trained from scratch. SD 2.0-v is a so-called v-prediction model.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\"> üî¥ This kernel explores the outcomes achievable by using the clip version (<a href=\"https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"><strong>CLIP-ViT-H-14-laion2B-s32B-b79K</strong></a>) that was utilized in the Stable Diffusion 2.0 training. It should be emphasized that the resulting output is heavily influenced by the set of text labels chosen to compare with the image embeddings.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\"> üî¥ CLIP interrogator's original version has <a href=\"https://github.com/pharmapsychotic/clip-interrogator/tree/main/clip_interrogator/data\"><strong>5 text label sets</strong></a>. When tested on the original version, I obtained a score of <b>0.44024</b>, but by simply removing the <b>artists.txt</b> and the <b>sites</b> labels lists, the score improved to <b>0.45836</b>. With experimentation using various sets and groups of lists, it is possible to achieve better performance than that of this kernel.</p>","metadata":{}},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n\n<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n\n* [1. About CLIP interrogator tool](#1)\n* [2. Install & Import all dependencies](#2)\n* [3. Set configs](#3)\n* [4. Load the Sample Submission](#4)\n* [5. Build index from images](#5)\n* [6. Load the embedding model](#6)\n* [7. Prepare CLIP interrogator tool](#7)\n    * [7.1 Define CLIP interrogator config](#7.1)\n    * [7.2 Define BLIP model](#7.2)\n    * [7.3 Define CLIP model](#7.3)\n    * [7.4 Create CLIP interrogator object](#7.4)\n* [8. Define interrogate function](#8)\n    * [8.1 Get labels embeddings](#8.1)\n    * [8.2 Create main interrogation function](#8.2)\n* [9. Extract promt from images](#9)\n    * [9.1 Check the result](#9.1)\n* [10. Create a sample submission with a constant prompt prediction](#10)\n    * [10.1 Encode prompts](#10.1)\n    * [10.2 Create submission DataFrame and save it as a .csv file](#10.2)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. About CLIP interrogator tool</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ The CLIP Interrogator is a prompt engineering tool that combines OpenAI's <a href=\"https://openai.com/blog/clip/\"><strong>CLIP</strong></a> and Salesforce's <a href=\"https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/\"><strong>BLIP</strong></a> to optimize text prompts to match a given image.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ CLIP Interrogator uses OpenCLIP which supports many different pretrained CLIP models. For the best prompts for Stable Diffusion 2.0 uses <b>ViT-H-14/laion2b_s32b_b79k</b>.</p>\n\n\n<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ CLIP Interrogator pipeline looks as follows:</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to BLIP to obtain the main description.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to CLIP to receive its embedding.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">Embeddings received from the image are compared with embeddings received from labels from the lists and the top 4 with the greatest similarity are selected.</p>\n<p style=\"font-family: consolas; font-size: 16px;\">There are 4 main lists on which the outgoing prompt for the CLIP part is formed: <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/artists.txt\"><strong>artists.txt</strong></a> (list with artists), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/flavors.txt\"><strong>flavors.txt</strong></a> (main list for image description), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/mediums.txt\"><strong>mediums.txt</strong></a> (image type), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/movements.txt\"><strong>movements.txt</strong></a> (image style) and <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L115\"><strong>sites</strong></a> (popular artwork sites). As I wrote earlier, removing the <b>artists.txt</b> and the <b>sites</b> lists can significantly improve the output score.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">The resulting texts are concatenated and returned as an image description (or promt on which an image was generated).</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">üî¥ CLIP Interrogator pipeline, schematic image [<a href=\"https://medium.com/@silkworm/diversify-photo-database-with-clip-interrogator-5dd1833be9f5\"><strong>source</strong></a>]:</p>\n\n![](https://user-images.githubusercontent.com/45982614/220214422-19529ba3-9c13-40cd-a3a6-434785002974.png)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ If you want, you can experiment with this tool on the Hugging Face Space -- *<a href=\"https://huggingface.co/spaces/pharma/CLIP-Interrogator\"><strong>Click</strong></a>*. Example of an output:</p>\n\n![](https://user-images.githubusercontent.com/45982614/220215304-d7e79716-35a2-4f29-867f-57ca996aab2a.png)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Install & Import all dependencies </b></div>","metadata":{}},{"cell_type":"code","source":"wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\nclip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\"","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:03.143442Z","iopub.execute_input":"2023-02-23T00:10:03.143847Z","iopub.status.idle":"2023-02-23T00:10:03.14888Z","shell.execute_reply.started":"2023-02-23T00:10:03.143812Z","shell.execute_reply":"2023-02-23T00:10:03.147877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-23T00:10:03.401599Z","iopub.execute_input":"2023-02-23T00:10:03.402179Z","iopub.status.idle":"2023-02-23T00:10:32.331498Z","shell.execute_reply.started":"2023-02-23T00:10:03.402143Z","shell.execute_reply":"2023-02-23T00:10:32.33031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nimport importlib\n\nfrom blip.models import blip\nfrom clip_interrogator import clip_interrogator","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-23T00:10:32.333608Z","iopub.execute_input":"2023-02-23T00:10:32.3343Z","iopub.status.idle":"2023-02-23T00:10:33.919238Z","shell.execute_reply.started":"2023-02-23T00:10:32.334245Z","shell.execute_reply":"2023-02-23T00:10:33.918056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace tokenizer path to prevent downloading\nblip_path = inspect.getfile(blip)\n\nfin = open(blip_path, \"rt\")\ndata = fin.read()\ndata = data.replace(\n    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n)\nfin.close()\n\nfin = open(blip_path, \"wt\")\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(blip)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-23T00:10:33.924633Z","iopub.execute_input":"2023-02-23T00:10:33.927596Z","iopub.status.idle":"2023-02-23T00:10:33.95288Z","shell.execute_reply.started":"2023-02-23T00:10:33.927522Z","shell.execute_reply":"2023-02-23T00:10:33.952024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fix clip_interrogator bug\nclip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n\nfin = open(clip_interrogator_path, \"rt\")\ndata = fin.read()\ndata = data.replace(\n    'open_clip.get_tokenizer(clip_model_name)', \n    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n)\nfin.close()\n\nfin = open(clip_interrogator_path, \"wt\")\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(clip_interrogator)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-23T00:10:33.957668Z","iopub.execute_input":"2023-02-23T00:10:33.960004Z","iopub.status.idle":"2023-02-23T00:10:33.988281Z","shell.execute_reply.started":"2023-02-23T00:10:33.959966Z","shell.execute_reply":"2023-02-23T00:10:33.987677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport open_clip\n\n\nsys.path.append('../input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models\n\ncomp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:33.989676Z","iopub.execute_input":"2023-02-23T00:10:33.990275Z","iopub.status.idle":"2023-02-23T00:10:35.038838Z","shell.execute_reply.started":"2023-02-23T00:10:33.990222Z","shell.execute_reply":"2023-02-23T00:10:35.037885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Set configs</b></div>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    device = \"cuda\"\n    seed = 42\n    embedding_length = 384\n    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n    clip_model_name = \"ViT-H-14\"\n    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n    cache_path = \"/kaggle/input/clip-interrogator-models-x\"","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:35.040347Z","iopub.execute_input":"2023-02-23T00:10:35.040734Z","iopub.status.idle":"2023-02-23T00:10:35.047391Z","shell.execute_reply.started":"2023-02-23T00:10:35.040696Z","shell.execute_reply":"2023-02-23T00:10:35.046294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Load the Sample Submission</b></div>","metadata":{}},{"cell_type":"code","source":"df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:35.04882Z","iopub.execute_input":"2023-02-23T00:10:35.049288Z","iopub.status.idle":"2023-02-23T00:10:35.082235Z","shell.execute_reply.started":"2023-02-23T00:10:35.049251Z","shell.execute_reply":"2023-02-23T00:10:35.081296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Build index from images</b></div>","metadata":{}},{"cell_type":"code","source":"images = os.listdir(comp_path / 'images')\nimgIds = [i.split('.')[0] for i in images]\n\neIds = list(range(CFG.embedding_length))\n\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, CFG.embedding_length),\n        np.tile(range(CFG.embedding_length), len(imgIds))\n    )\n]\n\nassert sorted(imgId_eId) == sorted(df_submission.index)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:35.084686Z","iopub.execute_input":"2023-02-23T00:10:35.085295Z","iopub.status.idle":"2023-02-23T00:10:35.09931Z","shell.execute_reply.started":"2023-02-23T00:10:35.085257Z","shell.execute_reply":"2023-02-23T00:10:35.098395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 6. Load the embedding model</b></div>","metadata":{}},{"cell_type":"code","source":"st_model = SentenceTransformer(CFG.sentence_model_path)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:35.100833Z","iopub.execute_input":"2023-02-23T00:10:35.101191Z","iopub.status.idle":"2023-02-23T00:10:36.634052Z","shell.execute_reply.started":"2023-02-23T00:10:35.101156Z","shell.execute_reply":"2023-02-23T00:10:36.632991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 7. Prepare CLIP interrogator tool</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.1 Define CLIP interrogator config</b></div>","metadata":{}},{"cell_type":"code","source":"model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\nmodel_config.cache_path = CFG.cache_path","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:36.639906Z","iopub.execute_input":"2023-02-23T00:10:36.640543Z","iopub.status.idle":"2023-02-23T00:10:36.647039Z","shell.execute_reply.started":"2023-02-23T00:10:36.640487Z","shell.execute_reply":"2023-02-23T00:10:36.646004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.2 Define BLIP model</b></div>","metadata":{}},{"cell_type":"code","source":"configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\nmed_config = os.path.join(configs_path, 'med_config.json')\nblip_model = blip.blip_decoder(\n    pretrained=CFG.blip_model_path,\n    image_size=model_config.blip_image_eval_size, \n    vit=model_config.blip_model_type, \n    med_config=med_config\n)\nblip_model.eval()\nblip_model = blip_model.to(model_config.device)\nmodel_config.blip_model = blip_model","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:10:36.648698Z","iopub.execute_input":"2023-02-23T00:10:36.652055Z","iopub.status.idle":"2023-02-23T00:11:10.924217Z","shell.execute_reply.started":"2023-02-23T00:10:36.651467Z","shell.execute_reply":"2023-02-23T00:11:10.923202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.3\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.3 Define CLIP model</b></div>","metadata":{}},{"cell_type":"code","source":"clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\nopen_clip.load_checkpoint(clip_model, CFG.clip_model_path)\nclip_model.to(model_config.device).eval()\nmodel_config.clip_model = clip_model","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:11:10.925576Z","iopub.execute_input":"2023-02-23T00:11:10.926582Z","iopub.status.idle":"2023-02-23T00:11:56.245463Z","shell.execute_reply.started":"2023-02-23T00:11:10.926537Z","shell.execute_reply":"2023-02-23T00:11:56.24406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_preprocess = open_clip.image_transform(\n    clip_model.visual.image_size,\n    is_train = False,\n    mean = getattr(clip_model.visual, 'image_mean', None),\n    std = getattr(clip_model.visual, 'image_std', None),\n)\nmodel_config.clip_preprocess = clip_preprocess","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:11:56.252968Z","iopub.execute_input":"2023-02-23T00:11:56.256163Z","iopub.status.idle":"2023-02-23T00:11:56.267398Z","shell.execute_reply.started":"2023-02-23T00:11:56.256107Z","shell.execute_reply":"2023-02-23T00:11:56.266187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.4\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.4 Create CLIP interrogator object</b></div>","metadata":{}},{"cell_type":"code","source":"ci = clip_interrogator.Interrogator(model_config)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:11:56.269159Z","iopub.execute_input":"2023-02-23T00:11:56.270096Z","iopub.status.idle":"2023-02-23T00:11:58.59092Z","shell.execute_reply.started":"2023-02-23T00:11:56.270059Z","shell.execute_reply":"2023-02-23T00:11:58.589996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 8. Define interrogate function</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.1 Get labels embeddings</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ Original CLIP Interrogator uses image_features and text_embeds matrix multiplication to fine the similarity between the corresponding image and text label. But I found that using cosine similarity is much faster and the resulting score is almost identical. So take that into account.</p>","metadata":{}},{"cell_type":"code","source":"cos = torch.nn.CosineSimilarity(dim=1)\n\nmediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\nmovements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\nflavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:11:58.59233Z","iopub.execute_input":"2023-02-23T00:11:58.593037Z","iopub.status.idle":"2023-02-23T00:11:59.149757Z","shell.execute_reply.started":"2023-02-23T00:11:58.592996Z","shell.execute_reply":"2023-02-23T00:11:59.148751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.2 Create main interrogation function</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">‚ö™ It's modified version of the original <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L213\"><strong>interrogate_classic</strong></a> method.</p>","metadata":{}},{"cell_type":"code","source":"def interrogate(image: Image) -> str:\n    caption = ci.generate_caption(image)\n    image_features = ci.image_to_features(image)\n    \n    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n\n    if caption.startswith(medium):\n        prompt = f\"{caption}, {movement}, {flaves}\"\n    else:\n        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n\n    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:11:59.15109Z","iopub.execute_input":"2023-02-23T00:11:59.151956Z","iopub.status.idle":"2023-02-23T00:11:59.160507Z","shell.execute_reply.started":"2023-02-23T00:11:59.151917Z","shell.execute_reply":"2023-02-23T00:11:59.159311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 9. Extract promt from images</b></div>","metadata":{}},{"cell_type":"code","source":"prompts = []\n\nimages_path = \"../input/stable-diffusion-image-to-prompts/images/\"\nfor image_name in images:\n    img = Image.open(images_path + image_name).convert(\"RGB\")\n\n    generated = interrogate(img)\n    \n    prompts.append(generated)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:12:14.487506Z","iopub.execute_input":"2023-02-23T00:12:14.487981Z","iopub.status.idle":"2023-02-23T00:12:18.996604Z","shell.execute_reply.started":"2023-02-23T00:12:14.487944Z","shell.execute_reply":"2023-02-23T00:12:18.99563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 9.1 Check the result</b></div>","metadata":{}},{"cell_type":"code","source":"def add_text_limiters(text: str) -> str:\n    return \" \".join([\n        word + \"\\n\" if i % 15 == 0 else word \n        for i, word in enumerate(text.split(\" \"), start=1)\n    ])\n\ndef plot_image(image: np.ndarray, original_prompt: str, generated_prompt: str) -> None:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    plt.annotate(\n        \"Original prompt:\\n\" + add_text_limiters(original_prompt) + \"\\n\\nGenerated prompt:\\n\" + add_text_limiters(generated_prompt), \n        xy=(1.05, 0.5), xycoords='axes fraction', ha='left', va='center', \n        fontsize=16, rotation=0, color=\"#104a6e\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:14:54.169731Z","iopub.execute_input":"2023-02-23T00:14:54.170108Z","iopub.status.idle":"2023-02-23T00:14:54.17806Z","shell.execute_reply.started":"2023-02-23T00:14:54.170077Z","shell.execute_reply":"2023-02-23T00:14:54.176673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT FORGET TO COMMENT OUT THIS CELL DURING SUBMISSION\n\noriginal_prompts_df = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n\nfor image_name, prompt in zip(images, prompts):\n    img = Image.open(images_path + image_name).convert(\"RGB\")\n    original_prompt = original_prompts_df[\n        original_prompts_df.imgId == image_name.split(\".\")[0]\n    ].prompt.iloc[0]\n    plot_image(img, original_prompt, prompt)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:14:54.406245Z","iopub.execute_input":"2023-02-23T00:14:54.40685Z","iopub.status.idle":"2023-02-23T00:14:57.544355Z","shell.execute_reply.started":"2023-02-23T00:14:54.406813Z","shell.execute_reply":"2023-02-23T00:14:57.54306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 10. Create a sample submission with a constant prompt prediction</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 10.1 Encode prompts</b></div>","metadata":{}},{"cell_type":"code","source":"prompt_embeddings = st_model.encode(prompts).flatten()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:15:02.212433Z","iopub.execute_input":"2023-02-23T00:15:02.212828Z","iopub.status.idle":"2023-02-23T00:15:02.302659Z","shell.execute_reply.started":"2023-02-23T00:15:02.212787Z","shell.execute_reply":"2023-02-23T00:15:02.301658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 10.2 Create submission DataFrame and save it as a .csv file</b></div>","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(\n    index=imgId_eId,\n    data=prompt_embeddings,\n    columns=['val']\n).rename_axis('imgId_eId')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:15:02.647388Z","iopub.execute_input":"2023-02-23T00:15:02.647791Z","iopub.status.idle":"2023-02-23T00:15:02.655776Z","shell.execute_reply.started":"2023-02-23T00:15:02.647751Z","shell.execute_reply":"2023-02-23T00:15:02.653512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T00:15:02.853839Z","iopub.execute_input":"2023-02-23T00:15:02.854182Z","iopub.status.idle":"2023-02-23T00:15:02.866999Z","shell.execute_reply.started":"2023-02-23T00:15:02.854153Z","shell.execute_reply":"2023-02-23T00:15:02.866036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ‡ºº‚Å† ‚Å†„Å§‚Å† ‚Å†‚óï‚Å†‚Äø‚Å†‚óï‚Å† ‚Å†‡ºΩ‚Å†„Å§ Thank You!</b></div>\n\n<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> üíå Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> üöÄ If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ‚ù§Ô∏è Once again, thank you for your support, and I hope to see you again soon!</p>","metadata":{}}]}