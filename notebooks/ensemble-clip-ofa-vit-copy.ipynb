{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About this notebook\nThis notebook is a copy of https://www.kaggle.com/code/motono0223/clipinterrogator-ofa-vit which ensembles the CLIP Interrogator, OFA model and ViT model.\n\n### Only change made is in the output of OFA model\nReplaced + \", fine details, masterpiece\" in OFA with remove_stopwords","metadata":{}},{"cell_type":"code","source":"# ratio_ViT_B_16          = 0.74880\n# ratio_CLIP_Interrogator = 0.21120\n# ratio_OFA               = 0.04000","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:48:12.46781Z","iopub.execute_input":"2023-04-18T21:48:12.468166Z","iopub.status.idle":"2023-04-18T21:48:12.474207Z","shell.execute_reply.started":"2023-04-18T21:48:12.468136Z","shell.execute_reply":"2023-04-18T21:48:12.47306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratio_ViT_B_16          = .75\nratio_CLIP_Interrogator = .2\nratio_OFA               = .05","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://arxiv.org/pdf/2202.03052.pdf\n\nOFA is an alogrithm developed by the R&D initiative of Alibaba Group.","metadata":{}},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> OFA </b></div>\n\n[Original notebook](https://www.kaggle.com/code/mayukh18/ofa-transformer-lb-0-42644)","metadata":{}},{"cell_type":"code","source":"# Using the pre compiled wheel since we don't have internet on submission\n!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:48:12.476764Z","iopub.execute_input":"2023-04-18T21:48:12.477198Z","iopub.status.idle":"2023-04-18T21:50:42.998339Z","shell.execute_reply.started":"2023-04-18T21:48:12.477161Z","shell.execute_reply":"2023-04-18T21:50:42.997102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom transformers import OFATokenizer, OFAModel\nfrom transformers.models.ofa.generate import sequence_generator\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:50:43.00165Z","iopub.execute_input":"2023-04-18T21:50:43.00268Z","iopub.status.idle":"2023-04-18T21:50:43.009517Z","shell.execute_reply.started":"2023-04-18T21:50:43.002637Z","shell.execute_reply":"2023-04-18T21:50:43.008373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\nIMAGE_DIR = \"/kaggle/input/stable-diffusion-image-to-prompts/images\"\n\nBATCH_SIZE = 24","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:50:43.012605Z","iopub.execute_input":"2023-04-18T21:50:43.013018Z","iopub.status.idle":"2023-04-18T21:50:43.024573Z","shell.execute_reply.started":"2023-04-18T21:50:43.012982Z","shell.execute_reply":"2023-04-18T21:50:43.023591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the pretrained OFA model","metadata":{}},{"cell_type":"code","source":"# Source: https://huggingface.co/OFA-Sys/ofa-large\n\n# used for image normalization\nmean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n\n# desired resolution for image resizing\nresolution = 480\n\n# chains together multiple data transformations\n# 1: converts the input image into RGB format\n# 2: resizes the image to desired resolution\n# 3: converts image to a PyTorch tensor\n# 4: normalizes the pixel values using the image normalization parameters from above\npatch_resize_transform = transforms.Compose([\n        lambda image: image.convert(\"RGB\"),\n        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n        transforms.ToTensor(), \n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n# customer tokenizer and custom model\n# OFA is a unified multimodal pretrained model that unifies modalities and tasks \n# to a simple sequence-to-sequence learning framework.\ntokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\nmodel = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\ntxt = \" what does the image describe?\"\n\n# the tokenized input text\ninputs = tokenizer([txt], return_tensors=\"pt\").input_ids","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:50:43.026251Z","iopub.execute_input":"2023-04-18T21:50:43.026699Z","iopub.status.idle":"2023-04-18T21:51:09.347072Z","shell.execute_reply.started":"2023-04-18T21:50:43.026665Z","shell.execute_reply":"2023-04-18T21:51:09.34603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model EDA","metadata":{}},{"cell_type":"code","source":"# displays images and their generated captions\n\n# retrieves the list of file paths\nsample_images = glob.glob(\"/kaggle/input/stable-diffusion-image-to-prompts/images/*\")[:7]\nfig, ax = plt.subplots(7,1, figsize=(4,35))\n\nfor i,impath in enumerate(sample_images):\n    # opens the image\n    image = Image.open(impath)\n    \n    # uses the data transformation function above\n    # sends the tensor to GPU and creates a batch of size 1\n    image_t = patch_resize_transform(image).cuda().unsqueeze(0)\n    \n    # generates text captions for the input image\n    # - tokenized input text and image are passed in (both are sent to GPU)\n    # - num_beams: controls the beam search\n    # - no_repeat_ngram_size: controls n-gram repetition during text generation\n    out = model.generate(inputs.cuda(), patch_images=image_t.cuda(), num_beams=5, no_repeat_ngram_size=2)\n    \n    # decodes the generated text captions from the model's output tensor\n    # skip_special_tokens: start-of-sentence and end-of-sentence tokens should be skipped\n    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n    \n    # display the image on the ith subplot\n    ax[i].imshow(image)\n    \n    # adds a text caption on the ith subplot\n    ax[i].text(1.1, .5, out_captions[0], horizontalalignment='left', verticalalignment='center', transform=ax[i].transAxes)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:09.348704Z","iopub.execute_input":"2023-04-18T21:51:09.349075Z","iopub.status.idle":"2023-04-18T21:51:17.612292Z","shell.execute_reply.started":"2023-04-18T21:51:09.349036Z","shell.execute_reply":"2023-04-18T21:51:17.611358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"sys.path.append('../input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models\n\ncomp_path = Path('../input/stable-diffusion-image-to-prompts/')\nst_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:17.613846Z","iopub.execute_input":"2023-04-18T21:51:17.62155Z","iopub.status.idle":"2023-04-18T21:51:18.583Z","shell.execute_reply.started":"2023-04-18T21:51:17.62151Z","shell.execute_reply":"2023-04-18T21:51:18.582008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generates image batches from a directory of image files\nclass ImageGen(Dataset):\n    def __init__(self, root, batch_size=32):\n        self.root = root\n        self.im_paths = os.listdir(self.root)\n        self.batch_size = batch_size\n        \n        # total number of images\n        self.sz = len(self.im_paths)\n        \n        # total number of batches to generate\n        self.genlen = self.sz//self.batch_size + int(self.sz%self.batch_size > 0)\n        \n    # retrieves an item from the dataset at a specific index\n    # returns a tuple containing the image batch and respective file IDs\n    def __getitem__(self, index):\n        \n        # checks if the given index is within bounds \n        if index >= self.genlen:\n            raise IndexError(\"Out of bounds\")\n        \n        # calculates start and end indices of image file paths using batch size and given index\n        l, r = index*self.batch_size, min(self.sz, (index+1)*self.batch_size)\n        \n        # creates a list of file paths, joining the root directory and image file paths\n        f_paths = [os.path.join(self.root, self.im_paths[i]) for i in range(l,r)]\n        \n        # creates a list of file IDs, by removing the file extension from the image file paths\n        f_ids = [self.im_paths[i][:-4] for i in range(l,r)]\n        \n        # same as above\n        ims = [Image.open(f_path) for f_path in f_paths]\n        ims = [patch_resize_transform(im).cuda().unsqueeze(0) for im in ims]\n        \n        # creates the final image batch tensor\n        ims = torch.cat(ims)\n        \n        # returns the image batch tensor and the file IDs, as a tuple\n        return ims, f_ids\n    \n    # returns the total # of batches to generate (which includes the entire length of the dataset)\n    def __len__(self):\n        return self.genlen","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:18.584666Z","iopub.execute_input":"2023-04-18T21:51:18.58503Z","iopub.status.idle":"2023-04-18T21:51:18.599529Z","shell.execute_reply.started":"2023-04-18T21:51:18.584994Z","shell.execute_reply":"2023-04-18T21:51:18.598464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generates image captions using a trained GPT-3 model\n# and then encodes the captions into embeddings using a pre-trained sentence-transformers model\n\n# will be used to remove common stopwords from the generated captions\nfrom gensim.parsing.preprocessing import remove_stopwords\n\n# will store caption IDs and corresponding embeddings\nsub_ids = []\nsub_embeds = []\n\n# uses the ImageGen class from above\nimgen = ImageGen(IMAGE_DIR, BATCH_SIZE)\n\n# iterates over the batches of image tensors\nfor b in imgen:\n    # iterates over the file IDs in the current batch\n    for j in range(len(b[1])):\n        \n        # 384 caption IDs for a given file ID\n        sub_ids.extend([f\"{b[1][j]}_{i}\" for i in range(384)])\n    \n    # retrieves the image batch tensor from the current batch \n    # contains resized and normalized image tensors\n    img_batch = b[0]\n    \n    # generates captions for the current image batch\n    out = model.generate(inputs.repeat(len(img_batch), 1).cuda(), patch_images=img_batch, num_beams=5, no_repeat_ngram_size=2)\n    \n    # decodes the generated captions from the model output tensor\n    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n    \n    # removes common stopwords from the generated captions\n    out_captions = [remove_stopwords(text) for text in out_captions]\n    \n    # encodes the cleaned captions into embeddings using a pre-trained sentence-transformers model\n    # flattens the resulting embeddings into a 1-D tensor\n    embeddings = st_model.encode(out_captions).flatten()\n    \n    # stores the embeddings of the corresponding captions\n    sub_embeds.extend(embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:18.604001Z","iopub.execute_input":"2023-04-18T21:51:18.604285Z","iopub.status.idle":"2023-04-18T21:51:25.015485Z","shell.execute_reply.started":"2023-04-18T21:51:18.604243Z","shell.execute_reply":"2023-04-18T21:51:25.014503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings1 = np.array(sub_embeds)\nembeddings1.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:25.017125Z","iopub.execute_input":"2023-04-18T21:51:25.017832Z","iopub.status.idle":"2023-04-18T21:51:25.025208Z","shell.execute_reply.started":"2023-04-18T21:51:25.017794Z","shell.execute_reply":"2023-04-18T21:51:25.024295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, tokenizer, st_model\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:25.026827Z","iopub.execute_input":"2023-04-18T21:51:25.02753Z","iopub.status.idle":"2023-04-18T21:51:25.558904Z","shell.execute_reply.started":"2023-04-18T21:51:25.027493Z","shell.execute_reply":"2023-04-18T21:51:25.557824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>CLIP Interrogator</b></div>\n\n[original notebook](https://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. About CLIP interrogator tool</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">âšª The CLIP Interrogator is a prompt engineering tool that combines OpenAI's <a href=\"https://openai.com/blog/clip/\"><strong>CLIP</strong></a> and Salesforce's <a href=\"https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/\"><strong>BLIP</strong></a> to optimize text prompts to match a given image.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">âšª CLIP Interrogator uses OpenCLIP which supports many different pretrained CLIP models. For the best prompts for Stable Diffusion 2.0 uses <b>ViT-H-14/laion2b_s32b_b79k</b>.</p>\n\n\n<p style=\"font-family: consolas; font-size: 16px;\">âšª CLIP Interrogator pipeline looks as follows:</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to BLIP to obtain the main description.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to CLIP to receive its embedding.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">Embeddings received from the image are compared with embeddings received from labels from the lists and the top 4 with the greatest similarity are selected.</p>\n<p style=\"font-family: consolas; font-size: 16px;\">There are 4 main lists on which the outgoing prompt for the CLIP part is formed: <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/artists.txt\"><strong>artists.txt</strong></a> (list with artists), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/flavors.txt\"><strong>flavors.txt</strong></a> (main list for image description), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/mediums.txt\"><strong>mediums.txt</strong></a> (image type), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/movements.txt\"><strong>movements.txt</strong></a> (image style) and <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L115\"><strong>sites</strong></a> (popular artwork sites). As I wrote earlier, removing the <b>artists.txt</b> and the <b>sites</b> lists can significantly improve the output score.</p>\n\n* <p style=\"font-family: consolas; font-size: 16px;\">The resulting texts are concatenated and returned as an image description (or promt on which an image was generated).</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">ðŸ”´ CLIP Interrogator pipeline, schematic image [<a href=\"https://medium.com/@silkworm/diversify-photo-database-with-clip-interrogator-5dd1833be9f5\"><strong>source</strong></a>]:</p>\n\n![](https://user-images.githubusercontent.com/45982614/220214422-19529ba3-9c13-40cd-a3a6-434785002974.png)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">âšª If you want, you can experiment with this tool on the Hugging Face Space -- *<a href=\"https://huggingface.co/spaces/pharma/CLIP-Interrogator\"><strong>Click</strong></a>*. Example of an output:</p>\n\n![](https://user-images.githubusercontent.com/45982614/220215304-d7e79716-35a2-4f29-867f-57ca996aab2a.png)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Install & Import all dependencies </b></div>","metadata":{}},{"cell_type":"code","source":"wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\nclip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\"","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:25.560449Z","iopub.execute_input":"2023-04-18T21:51:25.560803Z","iopub.status.idle":"2023-04-18T21:51:25.566025Z","shell.execute_reply.started":"2023-04-18T21:51:25.560767Z","shell.execute_reply":"2023-04-18T21:51:25.564896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-18T21:51:25.56781Z","iopub.execute_input":"2023-04-18T21:51:25.568916Z","iopub.status.idle":"2023-04-18T21:51:34.959246Z","shell.execute_reply.started":"2023-04-18T21:51:25.56888Z","shell.execute_reply":"2023-04-18T21:51:34.957857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list | grep transformers","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:34.965136Z","iopub.execute_input":"2023-04-18T21:51:34.967744Z","iopub.status.idle":"2023-04-18T21:51:57.916364Z","shell.execute_reply.started":"2023-04-18T21:51:34.967697Z","shell.execute_reply":"2023-04-18T21:51:57.915129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nimport importlib\n\nfrom blip.models import blip\nfrom clip_interrogator import clip_interrogator","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-18T21:51:57.918784Z","iopub.execute_input":"2023-04-18T21:51:57.919266Z","iopub.status.idle":"2023-04-18T21:51:57.926347Z","shell.execute_reply.started":"2023-04-18T21:51:57.919217Z","shell.execute_reply":"2023-04-18T21:51:57.925297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace tokenizer path to prevent downloading\nblip_path = inspect.getfile(blip)\n\nfin = open(blip_path, \"rt\")\ndata = fin.read()\n\n# uses the predownloaded tokenizer, instead of one from the internet\ndata = data.replace(\n    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n)\nfin.close()\n\nfin = open(blip_path, \"wt\")\n\n# updates the blip_path with the predownloaded tokenizer\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(blip)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-18T21:51:57.927852Z","iopub.execute_input":"2023-04-18T21:51:57.928332Z","iopub.status.idle":"2023-04-18T21:51:57.947997Z","shell.execute_reply.started":"2023-04-18T21:51:57.928272Z","shell.execute_reply":"2023-04-18T21:51:57.946864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fix clip_interrogator bug\nclip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n\nfin = open(clip_interrogator_path, \"rt\")\ndata = fin.read()\ndata = data.replace(\n    'open_clip.get_tokenizer(clip_model_name)', \n    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n)\nfin.close()\n\nfin = open(clip_interrogator_path, \"wt\")\nfin.write(data)\nfin.close()\n\n# reload module\nimportlib.reload(clip_interrogator)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-18T21:51:57.949596Z","iopub.execute_input":"2023-04-18T21:51:57.950078Z","iopub.status.idle":"2023-04-18T21:51:57.972214Z","shell.execute_reply.started":"2023-04-18T21:51:57.950041Z","shell.execute_reply":"2023-04-18T21:51:57.971323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \n\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# uses an open-source implementation of OpenAI's ClIP\n# https://github.com/mlfoundations/open_clip\nimport open_clip\n\n\nsys.path.append('../input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models\n\ncomp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:57.973425Z","iopub.execute_input":"2023-04-18T21:51:57.973855Z","iopub.status.idle":"2023-04-18T21:51:57.980348Z","shell.execute_reply.started":"2023-04-18T21:51:57.973819Z","shell.execute_reply":"2023-04-18T21:51:57.979205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Set configs</b></div>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    device = \"cuda\"\n    seed = 42\n    embedding_length = 384\n    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n    clip_model_name = \"ViT-H-14\"\n    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n    cache_path = \"/kaggle/input/clip-interrogator-models-x\"","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:57.98197Z","iopub.execute_input":"2023-04-18T21:51:57.983001Z","iopub.status.idle":"2023-04-18T21:51:57.990489Z","shell.execute_reply.started":"2023-04-18T21:51:57.982965Z","shell.execute_reply":"2023-04-18T21:51:57.98916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Load the Sample Submission</b></div>","metadata":{}},{"cell_type":"code","source":"df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:57.992124Z","iopub.execute_input":"2023-04-18T21:51:57.992816Z","iopub.status.idle":"2023-04-18T21:51:58.021493Z","shell.execute_reply.started":"2023-04-18T21:51:57.992782Z","shell.execute_reply":"2023-04-18T21:51:58.020286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Build index from images</b></div>","metadata":{}},{"cell_type":"code","source":"# code from competition organizers\nimages = os.listdir(comp_path / 'images')\nimgIds = [i.split('.')[0] for i in images]\n\neIds = list(range(CFG.embedding_length))\n\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, CFG.embedding_length),\n        np.tile(range(CFG.embedding_length), len(imgIds))\n    )\n]\n\nassert sorted(imgId_eId) == sorted(df_submission.index)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:58.022885Z","iopub.execute_input":"2023-04-18T21:51:58.023325Z","iopub.status.idle":"2023-04-18T21:51:58.037002Z","shell.execute_reply.started":"2023-04-18T21:51:58.023272Z","shell.execute_reply":"2023-04-18T21:51:58.035934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 6. Load the embedding model</b></div>","metadata":{}},{"cell_type":"code","source":"st_model = SentenceTransformer(CFG.sentence_model_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:58.038319Z","iopub.execute_input":"2023-04-18T21:51:58.039157Z","iopub.status.idle":"2023-04-18T21:51:58.372468Z","shell.execute_reply.started":"2023-04-18T21:51:58.039123Z","shell.execute_reply":"2023-04-18T21:51:58.371507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 7. Prepare CLIP interrogator tool</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.1 Define CLIP interrogator config</b></div>","metadata":{}},{"cell_type":"code","source":"model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\nmodel_config.cache_path = CFG.cache_path","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:58.374425Z","iopub.execute_input":"2023-04-18T21:51:58.374998Z","iopub.status.idle":"2023-04-18T21:51:58.39075Z","shell.execute_reply.started":"2023-04-18T21:51:58.374959Z","shell.execute_reply":"2023-04-18T21:51:58.389786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.2 Define BLIP model</b></div>","metadata":{}},{"cell_type":"code","source":"configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\nmed_config = os.path.join(configs_path, 'med_config.json')\nblip_model = blip.blip_decoder(\n    pretrained=CFG.blip_model_path,\n    image_size=model_config.blip_image_eval_size, \n    vit=model_config.blip_model_type, \n    med_config=med_config\n)\nblip_model.eval()\nblip_model = blip_model.to(model_config.device)\nmodel_config.blip_model = blip_model","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:51:58.39742Z","iopub.execute_input":"2023-04-18T21:51:58.397722Z","iopub.status.idle":"2023-04-18T21:52:25.893232Z","shell.execute_reply.started":"2023-04-18T21:51:58.397693Z","shell.execute_reply":"2023-04-18T21:52:25.892208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.3\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.3 Define CLIP model</b></div>","metadata":{}},{"cell_type":"code","source":"clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\nopen_clip.load_checkpoint(clip_model, CFG.clip_model_path)\nclip_model.to(model_config.device).eval()\nmodel_config.clip_model = clip_model","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:52:25.894641Z","iopub.execute_input":"2023-04-18T21:52:25.895012Z","iopub.status.idle":"2023-04-18T21:53:11.545055Z","shell.execute_reply.started":"2023-04-18T21:52:25.894973Z","shell.execute_reply":"2023-04-18T21:53:11.543995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_preprocess = open_clip.image_transform(\n    clip_model.visual.image_size,\n    is_train = False,\n    mean = getattr(clip_model.visual, 'image_mean', None),\n    std = getattr(clip_model.visual, 'image_std', None),\n)\nmodel_config.clip_preprocess = clip_preprocess","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:11.546693Z","iopub.execute_input":"2023-04-18T21:53:11.547113Z","iopub.status.idle":"2023-04-18T21:53:11.55426Z","shell.execute_reply.started":"2023-04-18T21:53:11.547075Z","shell.execute_reply":"2023-04-18T21:53:11.553377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7.4\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.4 Create CLIP interrogator object</b></div>","metadata":{}},{"cell_type":"code","source":"ci = clip_interrogator.Interrogator(model_config)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:11.55554Z","iopub.execute_input":"2023-04-18T21:53:11.556631Z","iopub.status.idle":"2023-04-18T21:53:13.800202Z","shell.execute_reply.started":"2023-04-18T21:53:11.556476Z","shell.execute_reply":"2023-04-18T21:53:13.799009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 8. Define interrogate function</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.1 Get labels embeddings</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">âšª Original CLIP Interrogator uses image_features and text_embeds matrix multiplication to fine the similarity between the corresponding image and text label. But I found that using cosine similarity is much faster and the resulting score is almost identical. So take that into account.</p>","metadata":{}},{"cell_type":"code","source":"# cosine similarity will be calculated across rows\ncos = torch.nn.CosineSimilarity(dim=1)\n\n# stacks tensors converted from numpy array (t was the numpy array)\nmediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\nmovements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\nflavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:13.801829Z","iopub.execute_input":"2023-04-18T21:53:13.802206Z","iopub.status.idle":"2023-04-18T21:53:14.306613Z","shell.execute_reply.started":"2023-04-18T21:53:13.802169Z","shell.execute_reply":"2023-04-18T21:53:14.30559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.2 Create main interrogation function</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">âšª It's modified version of the original <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L213\"><strong>interrogate_classic</strong></a> method.</p>","metadata":{}},{"cell_type":"code","source":"def interrogate(image: Image) -> str:\n    caption = ci.generate_caption(image)\n    image_features = ci.image_to_features(image)\n    \n    # grabs the top 1 image feature, text feature with the highesst cosine similarity for each text feature\n    # text feature: medium, movement, flaves\n    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n\n    if caption.startswith(medium):\n        prompt = f\"{caption}, {movement}, {flaves}\"\n    else:\n        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n\n    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:14.308095Z","iopub.execute_input":"2023-04-18T21:53:14.308475Z","iopub.status.idle":"2023-04-18T21:53:14.31575Z","shell.execute_reply.started":"2023-04-18T21:53:14.308438Z","shell.execute_reply":"2023-04-18T21:53:14.314687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 9. Extract promt from images</b></div>","metadata":{}},{"cell_type":"code","source":"prompts = []\n\nimages_path = \"../input/stable-diffusion-image-to-prompts/images/\"\nfor image_name in images:\n    img = Image.open(images_path + image_name).convert(\"RGB\")\n\n    generated = interrogate(img)\n    \n    prompts.append(generated)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:14.317082Z","iopub.execute_input":"2023-04-18T21:53:14.318193Z","iopub.status.idle":"2023-04-18T21:53:18.902963Z","shell.execute_reply.started":"2023-04-18T21:53:14.318157Z","shell.execute_reply":"2023-04-18T21:53:18.901924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 9.1 Check the result</b></div>","metadata":{}},{"cell_type":"code","source":"def add_text_limiters(text: str) -> str:\n    return \" \".join([\n        word + \"\\n\" if i % 15 == 0 else word \n        for i, word in enumerate(text.split(\" \"), start=1)\n    ])\n\ndef plot_image(image: np.ndarray, original_prompt: str, generated_prompt: str) -> None:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    plt.annotate(\n        \"Original prompt:\\n\" + add_text_limiters(original_prompt) + \"\\n\\nGenerated prompt:\\n\" + add_text_limiters(generated_prompt), \n        xy=(1.05, 0.5), xycoords='axes fraction', ha='left', va='center', \n        fontsize=16, rotation=0, color=\"#104a6e\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:18.904254Z","iopub.execute_input":"2023-04-18T21:53:18.904658Z","iopub.status.idle":"2023-04-18T21:53:18.912287Z","shell.execute_reply.started":"2023-04-18T21:53:18.904619Z","shell.execute_reply":"2023-04-18T21:53:18.911158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT FORGET TO COMMENT OUT THIS CELL DURING SUBMISSION\n\n# original_prompts_df = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n\n# for image_name, prompt in zip(images, prompts):\n#     img = Image.open(images_path + image_name).convert(\"RGB\")\n#     original_prompt = original_prompts_df[\n#        original_prompts_df.imgId == image_name.split(\".\")[0]\n#     ].prompt.iloc[0]\n#     plot_image(img, original_prompt, prompt)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:06:16.322922Z","iopub.execute_input":"2023-04-19T05:06:16.323365Z","iopub.status.idle":"2023-04-19T05:06:16.409707Z","shell.execute_reply.started":"2023-04-19T05:06:16.323272Z","shell.execute_reply":"2023-04-19T05:06:16.408184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 10. Create a sample submission with a constant prompt prediction</b></div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 10.1 Encode prompts</b></div>","metadata":{}},{"cell_type":"code","source":"embeddings2 = st_model.encode(prompts).flatten()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:18.924703Z","iopub.execute_input":"2023-04-18T21:53:18.925345Z","iopub.status.idle":"2023-04-18T21:53:19.004137Z","shell.execute_reply.started":"2023-04-18T21:53:18.92529Z","shell.execute_reply":"2023-04-18T21:53:19.003231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 11. Release GPU resource </b></div>","metadata":{}},{"cell_type":"code","source":"del ci\ndel blip_model, clip_model\ndel st_model\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.005488Z","iopub.execute_input":"2023-04-18T21:53:19.006535Z","iopub.status.idle":"2023-04-18T21:53:19.334214Z","shell.execute_reply.started":"2023-04-18T21:53:19.006498Z","shell.execute_reply":"2023-04-18T21:53:19.333295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> Ensemble(1) </b></div>\n","metadata":{}},{"cell_type":"code","source":"embeddings12 = ratio_OFA * embeddings1 + ratio_CLIP_Interrogator * embeddings2","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.33571Z","iopub.execute_input":"2023-04-18T21:53:19.336072Z","iopub.status.idle":"2023-04-18T21:53:19.342554Z","shell.execute_reply.started":"2023-04-18T21:53:19.336037Z","shell.execute_reply":"2023-04-18T21:53:19.341508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del embeddings1\ndel embeddings2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.344162Z","iopub.execute_input":"2023-04-18T21:53:19.344848Z","iopub.status.idle":"2023-04-18T21:53:19.561845Z","shell.execute_reply.started":"2023-04-18T21:53:19.344811Z","shell.execute_reply":"2023-04-18T21:53:19.560785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> ViT-B-16 </b></div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\nfrom sklearn.preprocessing import normalize","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.565618Z","iopub.execute_input":"2023-04-18T21:53:19.565897Z","iopub.status.idle":"2023-04-18T21:53:19.572467Z","shell.execute_reply.started":"2023-04-18T21:53:19.565865Z","shell.execute_reply":"2023-04-18T21:53:19.571328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_path = '/kaggle/input/stable-diffusion-vit-baseline-train/vit_base_patch16_224.pth'\n    model_name = 'vit_base_patch16_224'\n    input_size = 224\n    batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.573886Z","iopub.execute_input":"2023-04-18T21:53:19.57634Z","iopub.status.idle":"2023-04-18T21:53:19.583499Z","shell.execute_reply.started":"2023-04-18T21:53:19.576289Z","shell.execute_reply":"2023-04-18T21:53:19.582543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiffusionTestDataset(Dataset):\n    def __init__(self, images, transform):\n        self.images = images\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.images[idx])\n        image = self.transform(image)\n        return image","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.585068Z","iopub.execute_input":"2023-04-18T21:53:19.586133Z","iopub.status.idle":"2023-04-18T21:53:19.595177Z","shell.execute_reply.started":"2023-04-18T21:53:19.586097Z","shell.execute_reply":"2023-04-18T21:53:19.594283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    images,\n    model_path,\n    model_name,\n    input_size,\n    batch_size\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.RandomHorizontalFlip(p=0.5),\n#         transforms.RandomRotation(degrees=10),\n\n        # transforms.RandomVerticalFlip(p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    # uses the DiffusionTestDataset class from above\n    dataset = DiffusionTestDataset(images, transform)\n    dataloader = DataLoader(\n        dataset=dataset,\n        shuffle=False,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=2,\n        drop_last=False\n    )\n\n    model = timm.create_model(\n        model_name,\n        pretrained=False,\n        num_classes=384\n    )\n    state_dict = torch.load(model_path)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    \n    tta_preds = None\n    for _ in range(2):\n        preds = []\n        for X in tqdm(dataloader, leave=False):\n            X = X.to(device)\n\n            with torch.no_grad():\n                X_out = model(X).cpu().numpy()\n                # L2 normalize -- Start\n                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n                X_out = normalize( X_out )\n                # L2 normalize -- End\n                preds.append(X_out)\n                \n        if tta_preds is None:\n            tta_preds = np.vstack(preds).flatten()\n        else:\n            tta_preds += np.vstack(preds).flatten()\n    \n    return tta_preds / 2","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.596802Z","iopub.execute_input":"2023-04-18T21:53:19.597434Z","iopub.status.idle":"2023-04-18T21:53:19.609958Z","shell.execute_reply.started":"2023-04-18T21:53:19.597399Z","shell.execute_reply":"2023-04-18T21:53:19.608963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\nimgIds = [i.stem for i in images]\nEMBEDDING_LENGTH = 384\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, EMBEDDING_LENGTH),\n        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n\n# using the predict() from above\nembeddings3 = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:19.611352Z","iopub.execute_input":"2023-04-18T21:53:19.611934Z","iopub.status.idle":"2023-04-18T21:53:26.012968Z","shell.execute_reply.started":"2023-04-18T21:53:19.611898Z","shell.execute_reply":"2023-04-18T21:53:26.011882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> Ensemble(2) </b></div>","metadata":{}},{"cell_type":"code","source":"embeddings = embeddings12 + ratio_ViT_B_16 * embeddings3","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:26.016288Z","iopub.execute_input":"2023-04-18T21:53:26.017371Z","iopub.status.idle":"2023-04-18T21:53:26.023427Z","shell.execute_reply.started":"2023-04-18T21:53:26.01733Z","shell.execute_reply":"2023-04-18T21:53:26.022496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(\n    index=imgId_eId,\n    data=embeddings,\n    columns=['val']\n).rename_axis('imgId_eId')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:26.024825Z","iopub.execute_input":"2023-04-18T21:53:26.025332Z","iopub.status.idle":"2023-04-18T21:53:26.036066Z","shell.execute_reply.started":"2023-04-18T21:53:26.025275Z","shell.execute_reply":"2023-04-18T21:53:26.035147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:53:26.037768Z","iopub.execute_input":"2023-04-18T21:53:26.038125Z","iopub.status.idle":"2023-04-18T21:53:26.052723Z","shell.execute_reply.started":"2023-04-18T21:53:26.038091Z","shell.execute_reply":"2023-04-18T21:53:26.051732Z"},"trusted":true},"execution_count":null,"outputs":[]}]}